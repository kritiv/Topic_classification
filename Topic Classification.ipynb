{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = [row for row in csv.reader(open('data/reviews.csv'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Text', 'Sentiment', 'Topic'],\n",
       " ['The rooms are extremely small, practically only a bed.',\n",
       "  'negative',\n",
       "  'Comfort'],\n",
       " ['Room safe did not work.', 'negative', 'Facilities'],\n",
       " ['Mattress very comfortable.', 'positive', 'Comfort'],\n",
       " ['Very uncomfortable, thin mattress, with plastic cover that rustles every time you move.',\n",
       "  'negative',\n",
       "  'Comfort'],\n",
       " ['No bathroom in room', 'negative', 'Facilities'],\n",
       " ['The bed was soooo comfy.', 'positive', 'Comfort'],\n",
       " ['someone must have been smoking in the room next door.',\n",
       "  'negative',\n",
       "  'Cleanliness'],\n",
       " ['The bed is very comfortable.', 'positive', 'Comfort'],\n",
       " ['Very spacious rooms, quiet and very comfortable.', 'positive', 'Comfort']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\verma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Make all the strings lowercase and remove non alphabetic characters\n",
    "    text = re.sub('[^A-Za-z]', ' ', text.lower())\n",
    "\n",
    "    # Tokenize the text; this is, separate every sentence into a list of words\n",
    "    tokenized_text = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and stem each word to its root\n",
    "    clean_text = [\n",
    "        stemmer.stem(word) for word in tokenized_text\n",
    "        if word not in stopwords.words('english')\n",
    "    ]\n",
    "\n",
    "    # Remember, this final output is a list of words\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels ['Text', 'Sentiment', 'Topic'] removed\n",
    "reviews = review[1:]\n",
    "texts = [row[0] for row in reviews]    # list of texts\n",
    "topics = [row[2] for row in reviews]   # list of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The rooms are extremely small, practically only a bed.',\n",
       " 'Room safe did not work.',\n",
       " 'Mattress very comfortable.',\n",
       " 'Very uncomfortable, thin mattress, with plastic cover that rustles every time you move.',\n",
       " 'No bathroom in room']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing data and creating a string again from list returned by process_text()\n",
    "texts = [\" \".join(process_text(text)) for text in texts]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room extrem small practic bed',\n",
       " 'room safe work',\n",
       " 'mattress comfort',\n",
       " 'uncomfort thin mattress plastic cover rustl everi time move',\n",
       " 'bathroom room']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts must be vectorized \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(max_features=1000)\n",
    "vectors = matrix.fit_transform(texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207, 500)\n",
      "{'room': 354, 'extrem': 151, 'small': 388, 'practic': 319, 'bed': 40, 'safe': 359, 'work': 494, 'mattress': 265, 'comfort': 80, 'uncomfort': 464, 'thin': 432, 'plastic': 312, 'cover': 97, 'rustl': 358, 'everi': 142, 'time': 441, 'move': 276, 'bathroom': 38, 'soooo': 397, 'comfi': 79, 'someon': 395, 'must': 278, 'smoke': 392, 'next': 286, 'door': 119, 'spaciou': 403, 'quiet': 335, 'peopl': 306, 'bedroom': 41, 'sofa': 394, 'bit': 45, 'unconfort': 465, 'light': 244, 'common': 81, 'dim': 115, 'air': 12, 'condit': 84, 'fine': 164, 'type': 460, 'like': 245, 'let': 242, 'water': 483, 'run': 357, 'get': 182, 'wet': 488, 'take': 422, 'minut': 272, 'figur': 161, 'make': 260, 'hot': 209, 'gon': 187, 'na': 279, 'window': 492, 'singl': 379, 'glaze': 185, 'heat': 203, 'could': 94, 'escap': 137, 'although': 16, 'fair': 153, 'outsid': 298, 'terribl': 430, 'cubbyhol': 100, 'market': 264, 'corridor': 90, 'filthi': 163, 'electr': 128, 'cabl': 55, 'whole': 489, 'build': 53, 'smelli': 391, 'shower': 375, 'repuls': 344, 'wall': 478, 'seem': 364, 'sound': 399, 'insul': 221, 'gym': 193, 'basic': 36, 'springi': 407, 'unbeat': 461, 'show': 374, 'wear': 485, 'tear': 425, 'think': 434, 'didnt': 113, 'well': 486, 'tv': 457, 'open': 295, 'microwav': 268, 'need': 282, 'clean': 69, 'made': 258, 'iron': 225, 'hairdryer': 195, 'free': 172, 'coffe': 75, 'tea': 424, 'downstair': 121, 'area': 27, 'fluctuat': 169, 'felt': 160, 'littl': 249, 'draft': 122, 'last': 236, 'night': 288, 'fan': 155, 'realli': 340, 'loud': 255, 'cold': 76, 'renov': 342, 'pressur': 321, 'strong': 415, 'suitcas': 418, 'elev': 130, 'might': 269, 'challeng': 62, 'pay': 304, 'extra': 150, 'tini': 442, 'hip': 206, 'select': 365, 'pillow': 310, 'walk': 477, 'stori': 414, 'hallway': 197, 'stale': 409, 'feel': 159, 'construct': 86, 'across': 3, 'road': 351, 'noisi': 290, 'throughout': 435, 'day': 104, 'bathtub': 39, 'abund': 0, 'unit': 467, 'ice': 216, 'excel': 145, 'access': 2, 'fit': 165, 'facil': 152, 'especi': 138, 'pool': 316, 'real': 339, 'valu': 473, 'ad': 5, 'bare': 35, 'anyth': 24, 'els': 131, 'sleep': 384, 'lift': 243, 'pain': 300, 'card': 59, 'use': 471, 'counter': 96, 'bath': 37, 'restaur': 347, 'look': 254, 'invit': 224, 'compar': 82, 'rest': 346, 'hotel': 210, 'highli': 205, 'engin': 132, 'fix': 166, 'memori': 267, 'foam': 170, 'agre': 10, 'best': 42, 'ever': 141, 'toilet': 445, 'paper': 303, 'replac': 343, 'everyday': 143, 'greatest': 191, 'futon': 180, 'sleeper': 385, 'couch': 93, 'cramp': 98, 'cap': 58, 'sanitari': 360, 'bag': 33, 'avail': 30, 'freez': 173, 'stay': 411, 'seat': 363, 'love': 256, 'unbeliev': 462, 'smell': 390, 'larg': 235, 'would': 496, 'nice': 287, 'share': 368, 'restroom': 348, 'support': 420, 'mani': 263, 'stiff': 412, 'place': 311, 'amen': 20, 'slam': 383, 'mostli': 274, 'apart': 25, 'guest': 192, 'top': 447, 'dirti': 117, 'blind': 47, 'detest': 111, 'glass': 184, 'tub': 453, 'expect': 148, 'towel': 448, 'degre': 108, 'warmer': 482, 'winter': 493, 'rug': 356, 'shoe': 372, 'ac': 1, 'plug': 315, 'outlet': 297, 'surg': 421, 'protector': 330, 'give': 183, 'idea': 217, 'shutter': 377, 'go': 186, 'neither': 284, 'hang': 200, 'one': 294, 'side': 378, 'phone': 308, 'slow': 387, 'long': 253, 'line': 246, 'locat': 251, 'anda': 22, 'also': 15, 'garbag': 181, 'kitchen': 232, 'isntant': 226, 'packag': 299, 'left': 240, 'previou': 323, 'custom': 103, 'amaz': 18, 'roof': 352, 'terrac': 429, 'great': 190, 'aircon': 13, 'earplug': 125, 'slept': 386, 'soundli': 400, 'bright': 51, 'turn': 456, 'shine': 370, 'thru': 436, 'frost': 177, 'panel': 302, 'super': 419, 'cosi': 91, 'temperatur': 427, 'control': 87, 'mind': 270, 'stain': 408, 'sheet': 369, 'cockroach': 74, 'function': 179, 'poor': 317, 'pictur': 309, 'channel': 64, 'essenti': 139, 'allow': 14, 'workout': 495, 'quit': 336, 'price': 325, 'travel': 450, 'cot': 92, 'wifi': 491, 'connect': 85, 'everyth': 144, 'year': 497, 'came': 57, 'persian': 307, 'decor': 107, 'half': 196, 'heart': 202, 'length': 241, 'turkish': 455, 'bunt': 54, 'lantern': 234, 'bar': 34, 'ran': 337, 'jiggl': 229, 'handl': 199, 'linen': 247, 'wardrob': 481, 'space': 402, 'luggag': 257, 'chang': 63, 'proof': 328, 'thu': 437, 'fast': 157, 'never': 285, 'wait': 476, 'th': 431, 'floor': 168, 'citi': 67, 'nois': 289, 'bother': 50, 'around': 28, 'execel': 146, 'thing': 433, 'full': 178, 'size': 382, 'cm': 73, 'smaller': 389, 'spartan': 404, 'mean': 266, 'oher': 292, 'hr': 213, 'rooftop': 353, 'wallpap': 479, 'peek': 305, 'insuffici': 220, 'hard': 201, 'reach': 338, 'charg': 66, 'devic': 112, 'mous': 275, 'servic': 367, 'lock': 252, 'anymor': 23, 'main': 259, 'avenu': 31, 'pretti': 322, 'request': 345, 'queen': 334, 'call': 56, 'receiv': 341, 'shipment': 371, 'style': 416, 'king': 231, 'plenti': 314, 'spread': 406, 'unpack': 468, 'awesom': 32, 'extend': 149, 'charact': 65, 'hvac': 215, 'vent': 474, 'unclean': 463, 'neg': 283, 'old': 293, 'dust': 124, 'tissu': 443, 'hold': 207, 'told': 446, 'two': 459, 'doubl': 120, 'good': 188, 'whomev': 490, 'prior': 326, 'improv': 219, 'center': 61, 'soundproof': 401, 'showerroom': 376, 'con': 83, 'maker': 261, 'star': 410, 'qualiiti': 332, 'four': 171, 'werent': 487, 'english': 133, 'much': 277, 'potent': 318, 'pleasant': 313, 'febreez': 158, 'scent': 361, 'televis': 426, 'flat': 167, 'screen': 362, 'tube': 454, 'usual': 472, 'book': 49, 'far': 156, 'come': 78, 'yet': 498, 'famili': 154, 'adult': 7, 'soulless': 398, 'view': 475, 'immacul': 218, 'fresh': 174, 'big': 43, 'enough': 135, 'friend': 175, 'huge': 214, 'us': 470, 'still': 413, 'kettl': 230, 'boil': 48, 'tricki': 452, 'rins': 350, 'spotless': 405, 'classi': 68, 'front': 176, 'entranc': 136, 'curb': 101, 'appeal': 26, 'way': 484, 'retro': 349, 'hand': 198, 'narrow': 280, 'curtain': 102, 'dollar': 118, 'nasti': 281, 'qualiti': 333, 'privat': 327, 'adjust': 6, 'hour': 212, 'morn': 273, 'interior': 223, 'traffic': 449, 'leav': 239, 'easi': 126, 'pri': 324, 'tidi': 438, 'blanket': 46, 'heavi': 204, 'sink': 380, 'tap': 423, 'leak': 238, 'properli': 329, 'decent': 105, 'eleg': 129, 'art': 29, 'deco': 106, 'ambienc': 19, 'broken': 52, 'tile': 440, 'hotter': 211, 'rough': 355, 'shop': 373, 'underneath': 466, 'close': 70, 'late': 237, 'want': 480, 'dimli': 116, 'lit': 248, 'hole': 208, 'advertis': 8, 'com': 77, 'descript': 109, 'jet': 228, 'actual': 4, 'enjoy': 134, 'knew': 233, 'ahead': 11, 'tri': 451, 'someth': 396, 'differ': 114, 'count': 95, 'drape': 123, 'alway': 17, 'sit': 381, 'conveni': 88, 'exercis': 147, 'closest': 71, 'fill': 162, 'tight': 439, 'either': 127, 'yr': 499, 'intens': 222, 'minimalist': 271, 'cool': 89, 'design': 110, 'nyc': 291, 'lobbi': 250, 'carpet': 60, 'unstitch': 469, 'bigger': 44, 'even': 140, 'order': 296, 'prepaid': 320, 'paint': 301, 'sep': 366, 'manhattan': 262, 'ampl': 21, 'closet': 72, 'tend': 428, 'crumb': 99, 'snack': 393, 'hair': 194, 'got': 189, 'twin': 458, 'advis': 9, 'put': 331, 'togeth': 444, 'issu': 227, 'suit': 417}\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)\n",
    "print(matrix.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "vectors_train, vectors_test, topics_train, topics_test = train_test_split(vectors, topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " Cleanliness       0.50      0.30      0.37        10\n",
      "     Comfort       0.57      0.60      0.59        20\n",
      "  Facilities       0.56      0.64      0.60        22\n",
      "\n",
      "    accuracy                           0.56        52\n",
      "   macro avg       0.54      0.51      0.52        52\n",
      "weighted avg       0.55      0.56      0.55        52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(vectors_train, topics_train)\n",
    "\n",
    "# Predict with the testing set\n",
    "topics_pred = classifier.predict(vectors_test)\n",
    "\n",
    "# measure the accuracy of the results\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(topics_test, topics_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes, explained by Andrew Ng : https://www.youtube.com/watch?v=z5UQyCESW64\n",
    "Support Vector Machines, https://www.youtube.com/watch?v=N1vOgolbjSc\n",
    "Deep Learning explained simply in four parts https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
